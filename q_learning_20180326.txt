强化学习
无监督训练学习位置网络action的模型算法

q-learning 

agent   事物
reward  反馈
state   状态
action  行为
episode 探索

状态为行，行为为列，Reward矩阵R，其中-1表示空值（即节点之间没有边联通）
	0	1	2	3	4	5
0	-1	-1  -1  -1  0   -1
1	-1	-1  -1	0	-1	100
2	-1	-1	-1	0	-1	-1
3	-1	0	0	-1	0	-1
4	0	-1	-1	0	-1	100
5	-1	0	-1	-1	0	100

矩阵Q，他用来表示agent已经从经验中学到的知识。
矩阵Q和R是同阶，其行表示状态，列表示行为

Q初始化为零矩阵

Q-learning算法的转移规则（action怎么进行移动）
Q(s,a)=R(s,a)+gamma*max(Q(~s,~a))
其中，s,a表示当前的状态和行为,~s和~a表示下一个状态和行为，
gamma学习参数，满足0<=gamma<=1的常数

Q-learning计算过程
1.给定参数gamma和reward矩阵R
2.令Q=0
3.for each episode:
	3.1 随机选择一个初始化状态s
	3.2 若未达到目标状态，则执行以下几步
		（1）在当前状态s的所有可能行为中选取一个行为a
		（2）利用选定的行为a，得到下一个状态~s
		（3）按照公式计算Q(s,a)
		（4）令s=~state

搜索步骤
1.令当前状态s=s0
2.确定a,满足Q(s,a)=max{Q(s,~a)}
3.令当前状态s=~s(~s表示a对应的下一个状态)
4.重复2、3直到s为目标状态

首先取学习参数gamnma为0.8，初始化状态为房间1，将q初始化为一个零值矩阵
Q=0
s=1

观察矩阵R的第二行（对应房间1），它包含了2个非零的值，相当于状态1的下一个行为
有可能转到状态3或者状态5.随机的我们选择转到状态5
Q(1,5)=R(1,5)+gamma*max{Q(5,1),Q(5,4),Q(5,5)}
	  =100+0.8*max(0,0,0)
	  =100
Q
	0	1	2	3	4	5
0	0	0	0	0	0	0	
1	0	0	0	0	0	100
2	0	0	0	0	0	0	
3	0	0	0	0	0	0	
4	0	0	0	0	0	0	
5	0	0	0	0	0	0

s=3
	
Q(3,1)=R(3,1)+0.8*max{Q(1,3),Q(1,5)}
	  =0+0.8*max(0,100)
	  =80
Q
	0	1	2	3	4	5
0	0	0	0	0	0	0	
1	0	0	0	0	0	100
2	0	0	0	0	0	0	
3	0	80	0	0	0	0	
4	0	0	0	0	0	0	
5	0	0	0	0	0	0
Q(1,5)=R(1,5)+gamma*max{Q(5,1),Q(5,4),Q(5,5)}
	  =100+0.8*max(0,0,0)
	  =100

	0	1	2	3	4	5
0	0	0	0	0	400	0	
1	0	0	0	320	0	500
2	0	0	0	320	0	0	
3	0	400	256	0	400	0	
4	320	0	0	320	0	500	
5	0	400	0	0	400	500
对其规范化，每个非零元素都初一矩阵q的最大元素，也就是500，可以取出百分号，
相当于除以5

上述模式贪心搜索方式，只关心最大reward
q-learning普遍使用马尔可夫决策。 
	  